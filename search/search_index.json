{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AI Social Journal \ud83e\udde0\ud83d\udcac","text":"<p>Welcome to AI Social Journal, a personal blog where I document my journey through the world of Artificial Intelligence. This space is dedicated to learning, experimenting, and sharing insights as I explore the ever-evolving landscape of AI technologies.</p>"},{"location":"#why-this-journal","title":"Why This Journal?","text":"<p>As I dive deeper into AI \u2014 from building Retrieval-Augmented Generation (RAG) chatbots to experimenting with agentic assistants \u2014 I've found that writing is one of the best ways to reflect, solidify knowledge, and connect with like-minded learners. This journal serves three main purposes:</p> <ul> <li>\ud83d\udcda Learning in Public: Sharing what I learn as I learn it, including mistakes, insights, and breakthroughs.</li> <li>\ud83d\udee0\ufe0f Hands-On Tutorials: Step-by-step guides and practical examples based on real-world experiments.</li> <li>\ud83c\udf10 Building a Knowledge Base: A growing archive of AI concepts, tools, and best practices.</li> </ul>"},{"location":"#what-you-can-expect","title":"What You Can Expect","text":"<p>Here\u2019s what I\u2019ll be writing about:</p> <ul> <li> <p>\ud83e\udde9 Building AI Applications   From prototyping to production-ready solutions \u2014 tools, techniques, and lessons learned.</p> </li> <li> <p>\ud83d\uddc2\ufe0f RAG &amp; Vector Search   Diving deep into retrieval-augmented generation, embeddings, and vector databases.</p> </li> <li> <p>\ud83e\udd16 Agentic Workflows   Exploring how autonomous and semi-autonomous agents can be designed and orchestrated.</p> </li> <li> <p>\ud83e\uddea Experiment Logs   Documenting trials with new models, frameworks, and APIs \u2014 the wins and the failures.</p> </li> <li> <p>\ud83e\udde0 Theory into Practice   Simplifying complex AI concepts and showing how to use them in real-world scenarios.</p> </li> </ul>"},{"location":"#join-me-on-the-journey","title":"Join Me on the Journey","text":"<p>Whether you're new to AI, an experienced developer, or just curious about where this field is heading, I hope you find something useful, inspiring, or thought-provoking here.</p> <p>\ud83d\udee0\ufe0f Let\u2019s make sense of AI \u2014 together.</p> <p>Stay tuned for the first post!</p>"},{"location":"about/","title":"\ud83d\udc64 About Me","text":""},{"location":"about/#who-am-i","title":"Who Am I?","text":"<p>I\u2019m a technology leader passionate about innovation, intelligent systems, and the future of software. Over the years, I\u2019ve led high-impact teams, modernized legacy systems, and championed AI aided automation across enterprise environments. But beyond strategy and architecture, I\u2019ve always been a builder at heart \u2014 curious about how things work, and driven to make them better.</p> <p>Today, I\u2019m channeling that curiosity into AI \u2014 not just reading about it, but experimenting, prototyping, and learning by doing. This blog is where my technical journey intersects with my professional curiosity: a space where I share lessons, roadblocks, and discoveries as I explore what AI can really do in the hands of developers.</p>"},{"location":"about/#what-i-bring-to-the-table","title":"What I Bring to the Table","text":"<ul> <li> <p>\ud83e\udded Vision &amp; Strategy   Experience guiding enterprise-level technology transformations and aligning development with business outcomes.</p> </li> <li> <p>\ud83d\udee0\ufe0f Engineering Depth   From full-stack development to system architecture, I\u2019ve built scalable solutions that power real-world applications.</p> </li> <li> <p>\ud83e\udd16 AI Enthusiasm   Focused on LLMs, RAG pipelines, intelligent agents, and how AI can augment decision-making and development workflows.</p> </li> <li> <p>\ud83e\udde0 Lifelong Learning   Always evolving \u2014 learning in public, experimenting with emerging tools, and sharing it all along the way.</p> </li> </ul>"},{"location":"about/#why-i-started-this-blog","title":"Why I Started This Blog","text":"<p>I wanted a place to:</p> <ul> <li>\ud83d\udcd3 Reflect on what I\u2019m learning in the AI space</li> <li>\ud83e\uddea Share hands-on experiments and code</li> <li>\ud83d\udcac Connect with others building and learning in public</li> <li>\ud83d\udcda Build a living archive of what works, what doesn\u2019t, and why</li> </ul> <p>Whether you're a fellow developer, a curious technologist, or someone exploring AI for the first time \u2014 I hope you find something here that sparks your interest or helps you on your own journey.</p> <p>Thanks for stopping by. Let\u2019s build the future, one experiment at a time.</p>"},{"location":"posts/prompt-engineering/","title":"\ud83e\udde0 Prompt Engineering Deep Dive \u2013 Techniques, Tools &amp; Best Practices","text":""},{"location":"posts/prompt-engineering/#introduction","title":"\ud83d\udcd8 Introduction","text":"<p>Prompt engineering is the art and science of crafting inputs that guide Large Language Models (LLMs) to produce desired outputs. While anyone can write a prompt, effective prompt engineering requires an understanding of LLM behavior, configuration tuning, and iterative testing.</p> <p>Based on Google\u2019s 2024 whitepaper, this guide breaks down strategies, parameters, and real-world examples to help you get the most from any LLM.</p>"},{"location":"posts/prompt-engineering/#llm-configuration-essentials","title":"\u2699\ufe0f LLM Configuration Essentials","text":"Setting Description Tips Temperature Controls randomness in output Use <code>0</code> for deterministic, <code>0.9+</code> for creativity Top-K Sample from top K tokens Lower K for focus, higher for diversity Top-P Sample from top tokens within cumulative probability P 0.9\u20130.95 is a good balance Token Limit Controls max length of output Impacts cost and clarity <p>\u2705 Recommended defaults: <code>temperature=0.2</code>, <code>top-P=0.95</code>, <code>top-K=30</code>.</p>"},{"location":"posts/prompt-engineering/#prompting-techniques-with-examples","title":"\ud83e\uddea Prompting Techniques (with Examples)","text":""},{"location":"posts/prompt-engineering/#zero-shot-prompting","title":"\ud83d\udd39 Zero-Shot Prompting","text":"<p>Use: Simple tasks where the model can generalize well.</p> <p>Prompt:</p> <pre><code>Translate this to French: \"Where is the nearest restaurant?\"\n</code></pre>"},{"location":"posts/prompt-engineering/#one-shot-prompting","title":"\ud83d\udd39 One-Shot Prompting","text":"<p>Use: When the model needs guidance on format or tone.</p> <p>Prompt:</p> <pre><code>Example:\nInput: \"What is the capital of France?\"\nOutput: \"The capital of France is Paris.\"\n\nNow, answer this:\nInput: \"What is the capital of Japan?\"\n</code></pre>"},{"location":"posts/prompt-engineering/#few-shot-prompting","title":"\ud83d\udd39 Few-Shot Prompting","text":"<p>Use: Tasks with variability; adds consistency by showing patterns.</p> <p>Prompt:</p> <pre><code>Q: What is 5 + 3?\nA: 8\n\nQ: What is 12 - 4?\nA: 8\n\nQ: What is 9 + 6?\nA:\n</code></pre>"},{"location":"posts/prompt-engineering/#system-prompting","title":"\ud83d\udd39 System Prompting","text":"<p>Use: Guide output format, tone, or persona via system-level instruction.</p> <p>Prompt:</p> <pre><code>You are a JSON API assistant. Always respond in valid JSON format.\nUser: \"Tell me the current weather in London\"\n</code></pre>"},{"location":"posts/prompt-engineering/#role-prompting","title":"\ud83d\udd39 Role Prompting","text":"<p>Use: Assign a personality or function to steer response style.</p> <p>Prompt:</p> <pre><code>You are a helpful personal finance advisor.\nWhat\u2019s a good way to save for retirement in your 30s?\n</code></pre>"},{"location":"posts/prompt-engineering/#contextual-prompting","title":"\ud83d\udd39 Contextual Prompting","text":"<p>Use: Provide real data or background to ground the answer.</p> <p>Prompt:</p> <pre><code>Here\u2019s the company\u2019s 2023 HR policy: [insert excerpt]\n\nBased on this policy, can an employee carry over unused vacation days to next year?\n</code></pre>"},{"location":"posts/prompt-engineering/#advanced-prompting-strategies","title":"\ud83d\udd01 Advanced Prompting Strategies","text":""},{"location":"posts/prompt-engineering/#step-back-prompting","title":"\ud83d\udd38 Step-Back Prompting","text":"<p>Prompt:</p> <pre><code>Let\u2019s first reflect on the broader question:\n\"What factors should we consider before choosing a new CRM platform?\"\n\nNow, given those, which platform is best for a mid-sized SaaS startup?\n</code></pre>"},{"location":"posts/prompt-engineering/#chain-of-thought-cot","title":"\ud83d\udd38 Chain-of-Thought (CoT)","text":"<p>Prompt:</p> <pre><code>Q: Jane has 5 apples. She buys 7 more, then gives 3 to her friend. How many apples does she have now?\nA: Let's think step by step...\n</code></pre>"},{"location":"posts/prompt-engineering/#self-consistency","title":"\ud83d\udd38 Self-Consistency","text":"<p>Approach:</p> <ul> <li>Run the same prompt multiple times.</li> <li>Use majority voting to find the most consistent answer.</li> </ul> <p>Prompt (run 3x):</p> <pre><code>What\u2019s the next number in this sequence: 2, 4, 6, 8, ?\n</code></pre>"},{"location":"posts/prompt-engineering/#tree-of-thought-tot","title":"\ud83d\udd38 Tree-of-Thought (ToT)","text":"<p>Use: Let the model explore multiple branches of reasoning.</p> <p>Prompt:</p> <pre><code>You are solving a puzzle. First, generate 3 different strategies to solve it. Then evaluate which one is most effective and explain why.\n</code></pre>"},{"location":"posts/prompt-engineering/#react-reason-act","title":"\ud83d\udd38 ReAct (Reason + Act)","text":"<p>Use: Combine reasoning with tool use.</p> <p>Prompt:</p> <pre><code>User: What\u2019s the weather in Tokyo right now?\n\nAssistant Thought: I need to look up the weather using the weather API.\n\n[Call: GET https://api.weather.com/tokyo]\n\nAction: Retrieve weather info  \nObservation: It\u2019s 22\u00b0C and sunny  \nAnswer: The weather in Tokyo is currently 22\u00b0C with clear skies.\n</code></pre>"},{"location":"posts/prompt-engineering/#prompting-for-code-tasks","title":"\ud83d\udcbb Prompting for Code Tasks","text":"<p>Prompt engineering also works well with LLMs like Gemini or Claude for tasks like:</p> <ul> <li>Writing Bash scripts</li> <li>Explaining code</li> <li>Refactoring</li> <li>Translation (e.g., Python to JavaScript)</li> </ul> <p>Prompt Example:</p> <pre><code>Convert the following Python list comprehension to a standard for loop:\n[ x**2 for x in range(10) if x % 2 == 0 ]\n</code></pre>"},{"location":"posts/prompt-engineering/#best-practices-summary","title":"\u2705 Best Practices Summary","text":"<ul> <li>\ud83c\udfaf Be clear, concise, and direct</li> <li>\ud83e\uddf1 Use examples where helpful</li> <li>\ud83d\udcac Keep format structured</li> <li>\ud83e\uddea Test and iterate</li> <li>\ud83e\uddf0 Abstract common prompts with variables or templates</li> <li>\ud83d\udd12 Stay aligned with model safety and bias guidelines</li> </ul>"},{"location":"posts/prompt-engineering/#final-thoughts","title":"\ud83d\udccc Final Thoughts","text":"<p>Prompt engineering is your superpower when working with LLMs. It's part design, part trial-and-error, and part understanding the model\u2019s training behavior. With the right strategy, even complex workflows become simple, reusable, and reliable.</p>"},{"location":"posts/prompt-engineering/#references","title":"\ud83d\udcda References","text":"<ul> <li>Google Prompt Engineering Whitepaper (PDF)</li> <li>LangChain</li> <li>Vertex AI</li> </ul>"},{"location":"posts/prompt-engineering/#tags","title":"\ud83c\udff7 Tags","text":"<p><code>#PromptEngineering</code> <code>#LLMs</code> <code>#LangChain</code> <code>#ChainOfThought</code> <code>#TreeOfThought</code> <code>#ReActAgents</code> <code>#GenerativeAI</code> <code>#AIUX</code></p>"},{"location":"posts/rag-access-llms/","title":"\ud83e\udde0 RAG to Riches \u2013 Part 1: Accessing LLMs","text":"<p>Welcome to RAG to Riches, a series where we\u2019ll explore the key components involved in building a RAG (Retrieval-Augmented Generation) chatbot. Whether you're new to the space or looking to solidify your understanding, this series will walk through each building block \u2014 with hands-on code, real-world use cases, and lessons learned.</p>"},{"location":"posts/rag-access-llms/#what-is-rag","title":"\ud83e\udd16 What is RAG?","text":"<p>RAG, or Retrieval-Augmented Generation, is a method where we enhance Large Language Models (LLMs) by supplying them with external, domain-specific knowledge. Instead of training the model from scratch or fine-tuning it (which can be expensive and rigid), RAG pipelines inject relevant contextual data into the model at runtime \u2014 giving it the ability to answer questions with up-to-date, accurate, and business-specific information.</p>"},{"location":"posts/rag-access-llms/#traditional-ml-vs-llms","title":"\ud83e\udde0 Traditional ML vs LLMs","text":"<p>Traditional machine learning models are typically trained for narrow tasks \u2014 like classification, regression, or entity recognition. They're effective, but often require custom training and don't generalize well outside their training domain.</p> <p>LLMs, in contrast, are trained on vast corpora of internet-scale data and are capable of handling multiple tasks with little or no task-specific training. With techniques like RAG, we can guide these general-purpose models to perform focused tasks in a business context by injecting relevant knowledge into the prompt.</p>"},{"location":"posts/rag-access-llms/#why-rag","title":"\ud83d\udca1 Why RAG?","text":"<p>RAG is a practical, scalable way to:</p> <ul> <li>Introduce LLMs into business applications without extensive retraining</li> <li>Deliver consistent and accurate answers from a centralized knowledge base</li> <li>Build intelligent, context-aware conversational bots or assistants</li> </ul> <p>Common use cases include:</p> <ul> <li>Chatbots that answer internal documentation questions</li> <li>Customer service bots that respond using your product knowledge base</li> <li>Assistants that pull context-specific insights from reports or data stores</li> </ul>"},{"location":"posts/rag-access-llms/#components-of-a-rag-pipeline","title":"\ud83e\uddf1 Components of a RAG Pipeline","text":"<p>To build a fully functional RAG chatbot, the typical steps are:</p> <ol> <li>Identify and prepare business-specific data</li> <li>Generate embeddings from the data</li> <li>Store embeddings in a vector database</li> <li>Access an LLM to serve as the reasoning engine</li> <li>Retrieve relevant context and generate responses</li> </ol> <p>\ud83d\udc49 (We\u2019ll cover each of these steps in future posts.)</p>"},{"location":"posts/rag-access-llms/#focus-of-this-post-accessing-llms","title":"\ud83d\udd0d Focus of This Post: Accessing LLMs","text":"<p>Before we dive into embeddings or vector databases, we need to know how to access a foundational model \u2014 the heart of the RAG chatbot. Today, there are several ways to do that:</p>"},{"location":"posts/rag-access-llms/#major-providers","title":"\ud83d\udd0c Major Providers","text":"<ul> <li>Amazon Bedrock \u2013 Offers access to multiple models (Anthropic Claude, Meta Llama, Mistral, Cohere, etc.) through a single unified API.</li> <li>OpenAI \u2013 Creator of ChatGPT and GPT-4, provides models like <code>gpt-3.5-turbo</code> and <code>gpt-4</code> via API.</li> <li>Anthropic \u2013 Developer of the Claude series of models.</li> <li>Google Cloud \u2013 Offers access to Gemini and PaLM models through Vertex AI.</li> <li>Azure OpenAI \u2013 Microsoft\u2019s hosted OpenAI models on Azure infrastructure.</li> </ul> <p>Each of these services allows you to use powerful models through simple API calls \u2014 enabling integration into your app, chatbot, or internal tool.</p>"},{"location":"posts/rag-access-llms/#demo-accessing-a-model-via-amazon-bedrock","title":"\ud83e\uddea Demo: Accessing a Model via Amazon Bedrock","text":"<p>For demo purposes, we\u2019ll use Amazon Bedrock to access a foundational model and ask a few questions. Bedrock makes it easy to experiment with different models while abstracting away the infrastructure complexity.</p> <p>We\u2019ll write a simple Python script that:</p> <ul> <li>Connects to Bedrock using the AWS SDK (<code>boto3</code>)</li> <li>Selects a model (e.g., Claude v2 or Meta Llama 3)</li> <li>Sends a prompt and receives a response</li> </ul>"},{"location":"posts/rag-access-llms/#prerequisites","title":"\u2705 Prerequisites","text":"<ul> <li>AWS account with Bedrock access enabled</li> <li>IAM role or user with permissions to invoke models</li> <li>Python 3.9+ and <code>boto3</code> installed</li> </ul>"},{"location":"posts/rag-access-llms/#references","title":"\ud83d\udd17 References","text":"<p>In this post, we explored how to access large language models through services like Amazon Bedrock and OpenAI.Amazon Bedrock supports multiple foundational models like Claude and Titan, which you can explore in detail here.To start using these services, you'll also need to configure your AWS credentials.</p>"},{"location":"posts/rag-access-llms/#environment-setup","title":"\ud83d\udee0 Environment Setup","text":"<p>To follow along with the demo, let\u2019s set up a basic Python environment.</p>"},{"location":"posts/rag-access-llms/#create-a-virtual-environment","title":"\ud83d\udd27 Create a Virtual Environment","text":"<pre><code>python3 -m venv rag-env\nsource rag-env/bin/activate  # On Windows use: rag-env\\\\Scripts\\\\activate\n#install libraries\npip install boto3 botocore\n</code></pre>"},{"location":"posts/rag-access-llms/#setting-up-environment-variables-and-parameters","title":"\ud83d\udd10 Setting Up Environment Variables and Parameters","text":"<p>To securely access Amazon Bedrock and keep your code clean, it\u2019s good practice to use environment variables for configuration.</p>"},{"location":"posts/rag-access-llms/#required-aws-environment-variables","title":"\ud83e\uddfe Required AWS Environment Variables","text":"<p>You\u2019ll need to set the following in your terminal or through a <code>.env</code> file:</p> <pre><code>export AWS_ACCESS_KEY_ID=your-access-key-id\nexport AWS_SECRET_ACCESS_KEY=your-secret-access-key\nexport AWS_DEFAULT_REGION=your-region  # e.g., us-east-1\n</code></pre> Figure 1: High-level architecture of accessing FM using AWS Bedrock."},{"location":"posts/rag-access-llms/#code-block","title":"\ud83e\uddfe Code Block","text":"AWS <pre><code> \"\"\"\n    Calls the AWS Bedrock Claude Sonnet 4 model \n    with the given prompt.\n    Optionally accepts AWS access key and secret.\n    Returns the model's response as a string.\n    \"\"\"\nimport boto3\nimport json\n\n# Initialize the Bedrock runtime client\nbedrock = boto3.client(\n        \"bedrock-runtime\",\n        region_name=region,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n\n# Example prompt to the model\nprompt = \"What is Retrieval-Augmented Generation (RAG) in AI?\"\n\n# Invoke the model\nmodel_id = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n    native_request = {\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": 1000,\n        \"temperature\": 0.5,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"text\", \"text\": prompt}],\n            }\n        ],\n    }\n\nrequest = json.dumps(native_request)\nresponse = bedrock.invoke_model(modelId=model_id, body=request)\nmodel_response = json.loads(response[\"body\"].read())\nresponse_text = model_response[\"content\"][0][\"text\"]\n\n# Print the response\nprint(response_text)\n</code></pre> Langchain <pre><code>\"\"\"\n    Calls the AWS Bedrock Claude Sonnet 4 model using LangChain.\n    Optionally accepts AWS access key and secret.\n    Returns the model's response as a string.\n    \"\"\"\ntry:\n        model_id = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n        chat = ChatBedrock(\n            model_id=model_id,\n            region_name=region,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key\n        )\n        response = chat([HumanMessage(content=prompt)])\n        return response.content\n    except Exception as e:\n        print(f\"Error calling Claude Sonnet 3.7 (LangChain): {e}\")\n        return None\n</code></pre> <p>You can access the full code on GitHub here.</p>"},{"location":"posts/rag-access-llms/#output","title":"\ud83d\udcf8 Output","text":"<pre><code>Prompt:\nWhat is Retrieval-Augmented Generation (RAG) in AI?\n\nResponse:\nRetrieval-Augmented Generation (RAG) is a technique that combines the capabilities of large language models (LLMs) with external knowledge sources. Instead of relying solely on what the model was trained on, RAG retrieves relevant information from a vector database and incorporates it into the model\u2019s response generation process. This allows for more accurate, up-to-date, and contextually relevant outputs, especially in domain-specific applications.\n</code></pre> Error Handling: <p>Please make sure you have access to the models of your interest in your account and follow the documentation to format the request properly to avoid issues like the ones below:</p> <pre><code>botocore.errorfactory.AccessDeniedException: An error occurred (AccessDeniedException) when calling the InvokeModel operation: You don't have access to the model with the specified model ID.\n\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request, please reformat your input and try again.\n</code></pre>"},{"location":"posts/rag-access-llms/#summary","title":"\ud83e\uddfe Summary","text":"<p>In this post, we covered:</p> <ul> <li>What RAG is and why it\u2019s useful</li> <li>How LLMs differ from traditional models</li> <li>The steps involved in building a RAG system</li> <li>How to access a foundational model using Amazon Bedrock</li> </ul> <p>Accessing an LLM is the first foundational step in building a RAG pipeline. Once you\u2019ve connected to a model, you\u2019re ready to start injecting knowledge \u2014 and that\u2019s exactly what we\u2019ll explore next.</p>"},{"location":"posts/rag-access-llms/#coming-up-next","title":"\ud83d\udd2e Coming Up Next\u2026","text":"<p>In Part 2, we\u2019ll cover how to prepare your data and generate embeddings \u2014 turning your documents into searchable vectors that LLMs can understand and reason about.</p> <p>Tags: </p> <p><code>#AI</code> <code>#LLMs</code> <code>#LangChain</code> <code>#GenerativeAI</code> <code>#AmazonBedrock</code> <code>#RAG</code> <code>#Python</code> </p> <p>Stay tuned and follow along as we go from RAG\u2026 to riches.</p> <p>Posted on May 21, 2025</p>"},{"location":"posts/rag-data-setup/","title":"\ud83d\udcc4 RAG to Riches \u2013 Part 2: Preparing Data &amp; Generating Embeddings","text":"<p>In our previous post we explored how to access and interact with Large Language Models (LLMs) using providers like Amazon Bedrock. Now, we move on to the next step in building a RAG chatbot: preparing your data and generating vector embeddings.</p>"},{"location":"posts/rag-data-setup/#why-data-preparation-matters","title":"\ud83e\udde0 Why Data Preparation Matters","text":"<p>LLMs are powerful \u2014 trained on vast internet-scale datasets, they excel at understanding and generating human language. However, they lack context about your specific domain or internal data. That\u2019s where RAG comes in.</p> <p>To enable an LLM to reason over your data, you must first:</p> <ol> <li>Format it properly</li> <li>Convert it into machine-understandable vectors</li> <li>Store and retrieve it efficiently</li> </ol> <p>This step \u2014 preparing your data pipeline \u2014 is crucial for any RAG-based application.</p>"},{"location":"posts/rag-data-setup/#structured-vs-unstructured-data","title":"\ud83c\udfde Structured vs Unstructured Data","text":"<p>Today\u2019s data landscape is evolving. Traditional data warehouses are giving way to data lakes and lakehouses, handling both structured (CSV, SQL) and unstructured (PDFs, docs, text) content equally.</p> <p>In this series, we\u2019ll focus on unstructured data \u2014 PDFs, text files, etc. \u2014 and use LLMs to extract insights from them conversationally.</p> <p>\ud83d\udd0d Use Case: Think of a student prepping for exams using an LLM that answers questions from a textbook. That\u2019s RAG in action.</p>"},{"location":"posts/rag-data-setup/#fig-1-rag-architecture-with-data-pipeline","title":"\ud83d\uddbc Fig 1: RAG Architecture with Data Pipeline","text":""},{"location":"posts/rag-data-setup/#understanding-embeddings","title":"\ud83d\udd27 Understanding Embeddings","text":"<p>LLMs don\u2019t understand raw text. We must convert our unstructured data into embeddings \u2014 numerical representations that encode semantic meaning.</p> <p>Embeddings allow LLMs to \u201creason\u201d over text by comparing the vector similarity between user queries and stored knowledge.</p>"},{"location":"posts/rag-data-setup/#providers-of-embedding-models","title":"\ud83c\udfd7 Providers of Embedding Models","text":"<p>Some popular embedding providers include:</p> <ul> <li>Amazon Titan Embeddings</li> <li>OpenAI Embeddings</li> <li>Cohere</li> <li>Google Vertex AI</li> <li>HuggingFace models</li> </ul> <p>For this tutorial, we'll use Amazon Titan via Bedrock.</p> <p>\ud83d\udd17 Amazon Titan Embeddings on Bedrock</p>"},{"location":"posts/rag-data-setup/#building-the-data-pipeline","title":"\ud83e\uddf1 Building the Data Pipeline","text":"<p>Here\u2019s a high-level overview:</p> <ol> <li>Access raw document data</li> <li>Split documents into manageable chunks</li> <li>Convert chunks into embeddings</li> <li>Store embeddings in a vector database</li> <li>Search embeddings by query</li> <li>Feed relevant chunks into an LLM for a final answer</li> </ol>"},{"location":"posts/rag-data-setup/#tools-used","title":"\ud83d\udee0 Tools Used","text":"<ul> <li>LangChain: For document splitting and LLM abstraction</li> <li>FAISS: In-memory vector database for fast retrieval</li> <li>Bedrock: AWS Bedrock, serverless service for accessing different FMs </li> </ul> <p>Using LangChain for Rapid Development</p> <p>For this demo, we\u2019ll be using LangChain, an open-source framework for building LLM-based applications. LangChain simplifies the complexity of accessing LLMs by providing convenient abstractions and wrappers, allowing developers to build powerful applications faster and reduce time to market.</p>"},{"location":"posts/rag-data-setup/#step-1-splitting-text-into-chunks","title":"\u2702\ufe0f Step 1: Splitting Text into Chunks","text":"<pre><code>def create_chunks(self, documents: List, \n                    chunk_size: int = 1000, \n                    chunk_overlap: int = 200) -&gt; List:\n    \"\"\"\n    Split documents into chunks with error handling\n\n    Args:\n        documents: List of documents to chunk\n        chunk_size: Size of each chunk\n        chunk_overlap: Overlap between chunks\n\n    Returns:\n        List of document chunks\n    \"\"\"\n    try:\n        if not documents:\n            logger.warning(\"No documents provided for chunking\")\n            return []\n\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=chunk_size,\n            chunk_overlap=chunk_overlap,\n            length_function=len,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n        chunks = text_splitter.split_documents(documents)\n        logger.info(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n\n        return chunks\n\n    except Exception as e:\n        logger.error(f\"Error creating chunks: {e}\")\n        return []\n</code></pre> <p>\u2705 Recommended:</p> <ul> <li><code>chunk_size</code>: 500\u20131000 characters</li> <li><code>chunk_overlap</code>: 10\u201320% of chunk size</li> </ul> <p>This ensures context is preserved across chunks.</p>"},{"location":"posts/rag-data-setup/#step-2-generating-embeddings","title":"\ud83d\udd22 Step 2: Generating Embeddings","text":"<pre><code>def _get_bedrock_embeddings(self) -&gt; Optional[BedrockEmbeddings]:\n    \"\"\"Initialize and return Bedrock embeddings with error handling\"\"\"\n    try:\n        # Get AWS credentials\n        aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n        aws_secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n\n        embeddings_kwargs = {\n            \"model_id\": self.embedding_model_id,\n            \"region_name\": self.region\n        }\n\n        # Add credentials if available\n        if aws_access_key_id and aws_secret_access_key:\n            embeddings_kwargs.update({\n                \"credentials_profile_name\": None  # Use explicit credentials\n            })\n\n        embeddings = BedrockEmbeddings(**embeddings_kwargs)\n        logger.info(f\"Initialized Bedrock embeddings with model: {self.embedding_model_id}\")\n        return embeddings\n\n    except Exception as e:\n        logger.error(f\"Failed to initialize Bedrock embeddings: {e}\")\n        return None\n</code></pre>"},{"location":"posts/rag-data-setup/#step-3-storing-in-vector-db","title":"\ud83d\uddc3 Step 3: Storing in Vector DB","text":"<pre><code># Create vector store\nlogger.info(\"Creating FAISS vector store...\")\nvector_store = FAISS.from_documents(\n    documents=chunks,\n    embedding=self.embeddings\n)\n\n# Save vector store\nos.makedirs(os.path.dirname(vector_db_path) if os.path.dirname(vector_db_path) else '.', exist_ok=True)\nvector_store.save_local(vector_db_path)\nlogger.info(f\"FAISS vector database created and saved to {vector_db_path}\")\n</code></pre> <p>\ud83e\udde0 Other vector DBs to explore:</p> <ul> <li>ChromaDB</li> <li>Weaviate</li> <li>Pinecone</li> <li>Amazon OpenSearch</li> </ul>"},{"location":"posts/rag-data-setup/#step-4-querying-the-vector-db","title":"\ud83d\udd0d Step 4: Querying the Vector DB","text":"<pre><code># Load vector database if not provided\nif vector_db is None:\n    vector_db = self.load_vector_db(vector_db_path)\n    if vector_db is None:\n        return \"Failed to load vector database. Please ensure the database exists and is accessible.\"\n\n# Search for relevant documents\nlogger.info(f\"Searching for relevant documents for question: {question[:100]}...\")\ndocs = vector_db.similarity_search(question, k=k)\n</code></pre>"},{"location":"posts/rag-data-setup/#our-data-is-now-searchable","title":"\u2705 Our Data is Now Searchable!","text":"<p>Now that our embeddings are stored and indexed, we can query them \u2014 and pass the results to an LLM to generate a contextual answer.</p>"},{"location":"posts/rag-data-setup/#example-querying-documents","title":"\ud83e\uddea Example: Querying Documents","text":"<p>To demonstrate the pipeline, I\u2019ve selected Google\u2019s prompt engineering whitepaper as the input document \u2014 a detailed resource outlining advanced prompting strategies.</p> <p>Document Source: Google\u2019s Prompt Engineering Whitepaper (PDF)</p> <pre><code># Prepare context\ncontext = \"\\n\\n\".join([doc.page_content for doc in docs])\n\n# Create prompt\nprompt = f\"\"\"You are a helpful assistant that answers questions based on the provided context from PDF documents.\n\nContext:\n{context}\n\nQuestion: {question}\n\nInstructions:\n- Answer the question based only on the provided context\n- If the context doesn't contain enough information, say \"I don't have enough information to answer this question based on the provided documents\"\n- Be concise but comprehensive in your answer\n- Cite relevant parts of the context when possible\n\nAnswer:\"\"\"\n\n# Get answer from Claude\nlogger.info(\"Getting answer from Bedrock Claude...\")\nanswer = self.call_bedrock_claude(prompt)\n\nreturn answer\n</code></pre>"},{"location":"posts/rag-data-setup/#output","title":"\ud83d\udcf8 Output","text":"<p>Prompt:</p> <p>\"What is a prompt?\"</p> <p>Output:</p> <p>Based on the provided context, a prompt is an input given to a Large Language Model (LLM) to generate a response or prediction. When you write a prompt, you are trying to guide the LLM to predict the right sequence of tokens. The context specifically states that \"in the context of natural language processing and LLMs, a prompt is an input provided to the model to generate a response or prediction.</p> <p>Prompts can take different forms, including: - Zero-shot prompts (which provide just a description of a task without examples) - Role prompts (where the AI is assigned a specific role) - Multimodal prompts (which can combine text, images, audio, code, or other formats)</p> <p>The effectiveness of a prompt often requires tinkering and optimization of factors like length, writing style, and structure in relation to the task at hand.\"</p> Figure 2: Query response from the RAG application using AWS Bedrock. <p>You can access the full code on GitHub here.</p>"},{"location":"posts/rag-data-setup/#summary","title":"\ud83d\udccc Summary","text":"<p>In this post, we covered:</p> <ul> <li>Why unstructured data matters in RAG</li> <li>What embeddings are and how they power LLM search</li> <li>How to split, embed, and store your documents</li> <li>How to query the knowledge base using FAISS</li> </ul>"},{"location":"posts/rag-data-setup/#next-up","title":"\ud83d\udd2e Next Up","text":"<p>In Part 3, we\u2019ll explore retrieval and generation \u2014 using your vector database + LLM to answer user queries interactively and **prompt engineering techniques.</p> <p>Subscribe or follow to continue your RAG journey \ud83d\ude80</p>"},{"location":"posts/rag-data-setup/#tags","title":"\ud83d\udd16 Tags","text":"<p><code>#RAG</code> <code>#LLMs</code> <code>#LangChain</code> <code>#GenerativeAI</code> <code>#AmazonBedrock</code> <code>#PromptEngineering</code> <code>#AI</code> <code>#MachineLearning</code> <code>#DataPipeline</code></p> <p>Published: May 31, 2025</p>"}]}